mul_intrinsic(float4 const*, float4 const&):
	movaps	(%rsi), %xmm0
	movaps	%xmm0, %xmm2
	movaps	%xmm0, %xmm3
	movaps	%xmm0, %xmm1
	shufps	$0, %xmm0, %xmm2
	shufps	$85, %xmm0, %xmm3
	shufps	$170, %xmm0, %xmm1
	mulps	(%rdi), %xmm2
	shufps	$255, %xmm0, %xmm0
	mulps	16(%rdi), %xmm3
	mulps	48(%rdi), %xmm0
	mulps	32(%rdi), %xmm1
	addps	%xmm3, %xmm2
	addps	%xmm0, %xmm1
	addps	%xmm1, %xmm2
	movaps	%xmm2, -24(%rsp)
	movq	-16(%rsp), %rax
	movq	-24(%rsp), %xmm0
	movd	%rax, %xmm1
	ret
mul_cpp(float4 const*, float4 const&):
	movss	(%rsi), %xmm3
	movss	4(%rsi), %xmm6
	movss	12(%rdi), %xmm0
	movss	28(%rdi), %xmm1
	mulss	%xmm3, %xmm0
	movss	8(%rsi), %xmm5
	mulss	%xmm6, %xmm1
	movss	24(%rdi), %xmm2
	movss	20(%rdi), %xmm9
	mulss	%xmm6, %xmm2
	movss	12(%rsi), %xmm4
	mulss	%xmm6, %xmm9
	movss	60(%rdi), %xmm7
	addss	%xmm1, %xmm0
	movss	44(%rdi), %xmm1
	mulss	16(%rdi), %xmm6
	mulss	%xmm5, %xmm1
	movss	56(%rdi), %xmm8
	mulss	%xmm4, %xmm7
	mulss	%xmm4, %xmm8
	addss	%xmm1, %xmm0
	movss	8(%rdi), %xmm1
	mulss	%xmm3, %xmm1
	addss	%xmm7, %xmm0
	addss	%xmm2, %xmm1
	movss	40(%rdi), %xmm2
	movd	%xmm0, %rcx
	salq	$32, %rcx
	mulss	%xmm5, %xmm2
	addss	%xmm2, %xmm1
	movss	4(%rdi), %xmm2
	mulss	%xmm3, %xmm2
	mulss	(%rdi), %xmm3
	addss	%xmm8, %xmm1
	addss	%xmm9, %xmm2
	movss	36(%rdi), %xmm9
	addss	%xmm6, %xmm3
	mulss	%xmm5, %xmm9
	mulss	32(%rdi), %xmm5
	addss	%xmm9, %xmm2
	movss	52(%rdi), %xmm9
	addss	%xmm5, %xmm3
	mulss	%xmm4, %xmm9
	mulss	48(%rdi), %xmm4
	addss	%xmm9, %xmm2
	addss	%xmm3, %xmm4
	movd	%xmm2, %rax
	movd	%xmm4, %edx
	salq	$32, %rax
	movl	%edx, %edx
	orq	%rax, %rdx
	movd	%xmm1, %eax
	movl	%eax, %eax
	movd	%rdx, %xmm0
	orq	%rcx, %rax
	movd	%rax, %xmm1
	ret
mul_inst_like(float4 const*, float4 const&):
	movss	(%rsi), %xmm3
	movss	4(%rsi), %xmm2
	movss	12(%rdi), %xmm5
	movss	28(%rdi), %xmm4
	mulss	%xmm3, %xmm5
	movss	8(%rsi), %xmm1
	mulss	%xmm2, %xmm4
	movss	12(%rsi), %xmm0
	movss	60(%rdi), %xmm6
	movss	8(%rdi), %xmm7
	mulss	%xmm0, %xmm6
	movss	56(%rdi), %xmm8
	mulss	%xmm4, %xmm5
	movss	44(%rdi), %xmm4
	mulss	%xmm3, %xmm7
	movss	4(%rdi), %xmm9
	mulss	%xmm1, %xmm4
	movss	52(%rdi), %xmm10
	mulss	%xmm0, %xmm8
	mulss	%xmm3, %xmm9
	mulss	%xmm0, %xmm10
	mulss	%xmm6, %xmm4
	movss	24(%rdi), %xmm6
	mulss	48(%rdi), %xmm0
	mulss	%xmm2, %xmm6
	mulss	(%rdi), %xmm3
	mulss	%xmm5, %xmm4
	mulss	%xmm6, %xmm7
	movss	40(%rdi), %xmm6
	mulss	%xmm1, %xmm6
	movd	%xmm4, %rcx
	salq	$32, %rcx
	mulss	%xmm8, %xmm6
	movss	20(%rdi), %xmm8
	mulss	%xmm2, %xmm8
	mulss	16(%rdi), %xmm2
	mulss	%xmm7, %xmm6
	mulss	%xmm8, %xmm9
	movss	36(%rdi), %xmm8
	mulss	%xmm2, %xmm3
	mulss	%xmm1, %xmm8
	mulss	32(%rdi), %xmm1
	mulss	%xmm10, %xmm8
	mulss	%xmm0, %xmm1
	mulss	%xmm9, %xmm8
	mulss	%xmm3, %xmm1
	movd	%xmm8, %rax
	movd	%xmm1, %edx
	salq	$32, %rax
	movl	%edx, %edx
	orq	%rax, %rdx
	movd	%xmm6, %eax
	movl	%eax, %eax
	movd	%rdx, %xmm0
	orq	%rcx, %rax
	movd	%rax, %xmm1
	ret
main:
	xorl	%eax, %eax
	ret